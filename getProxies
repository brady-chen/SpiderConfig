# -*- coding:utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf8')
from bs4 import BeautifulSoup
import time, requests, re, threading



headers = {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
           'Accept-Encoding': 'gb2312,utf-8',#这里原先为gzip, deflate, sdch
           'Accept-Language': 'zh-CN,zh;q=0.8',
           'Connection': 'keep-alive',
           'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36'}

class Proxies(threading.Thread):
    """
    一、初始化Proxies类时则自动获取代理数据
    二、输入一个url列表到函数getIpAndPortInList验证并筛选优质ip，若url列表为空，则按默认测试网址测试ip
    三、返回ip_and_prot字符串作为代理
    """
    def __init__(self):
        print "已启用ip获取器，程序运行中......"
        self.page_num = 1
        self.START_URL = "http://www.xicidaili.com/nn/"
        self.urls = []
        self.htmls = []
        self.list_ip = []
        self.list_port = []
        self.list_address = []
        self.isAnonymityOrNot = True
        self.list_type = []
        self.list_internet_speed = []
        self.list_connect_time = []
        self.list_survival_time = []
        self.list_check_date = []
        self.ip_and_port = " "
        self.get_all_data()



    def get_url(self, page_num=2):
        self.urls.append(self.START_URL)
        for num in range(1, page_num):
            self.urls.append(self.START_URL+str(num))
        #return self.urls


    def get_Html(self):
        session = requests.session()
        session.headers = headers
        for url in self.urls:
            try:
                request = session.get(url)
            except IndexError, e:
                print "出错于getHtml()，遍历错误。具体原因为："+str(e)
            except requests.ConnectionError, e:
                print "出错于getHtml()，网络问题异常，如DNS查询失败、拒绝连接等。具体原因为:"+str(e)
            except requests.TooManyRedirects, e:
                print "出错于getHtml()，超过了设定的最大重定向次数。具体原因为:"+str(e)
            except requests.HTTPError, e:
                print "出错于getHtml()，非200成功状态码。具体原因为:"+str(e)
            except requests.RequestException, e:
                print "出错于getHtml()，requests的父异常。具体原因为:"+str(e)
            except BaseException, e:
                print "出错于getHtml()，程序父错误。具体原因为:"+str(e)
            else:
                if request.status_code == 404:
                    print "出错于getHtml()，404错误，不存在此页面"
                else:
                    soup = BeautifulSoup(request.content, 'lxml')
                    clean_html = soup.findAll({"tr"})
                    for html in clean_html:
                        self.htmls.append(html)
        #return self.htmls


    def get_ip(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                ip = soup.find("tr").findAll("td")[1].get_text()
                # port = soup.find("tr").findAll("td")[2].get_text()
                # address = str(soup.find("tr").findAll("td")[3].get_text()).replace("\n","")
                # isAnonymityOrNot = soup.find("tr").findAll("td")[4].get_text()
                # type = soup.find("tr").findAll("td")[5].get_text()
                # internet_speed = soup.find("tr").findAll("td")[6].find({"div", "title"}).attrs["title"]
                # connect_time = soup.find("tr").findAll("td")[7].find({"div", "title"}).attrs["title"]
                # survival_time = soup.find("tr").findAll("td")[8].get_text()
                # check_date = soup.find("tr").findAll("td")[9].get_text()
            except IndexError, e:
                #print "出错于get_ip(),遍历错误。具体原因为："+str(e)
                pass
            # except BaseException, e:
            #     print "出错于get_ip(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_ip.append(ip)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip"

                # a.append(str(soup.get_text()).replace('\n', ''))
                # if a[-1] == u"国家IP地址端口服务器地址是否匿名类型速度连接时间存活时间验证时间":
                #     pass
                # else:
                #     print a[-1]

        #print a

    def get_ip_port(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                port = soup.find("tr").findAll("td")[2].get_text()
            except IndexError, e:
                #print "出错于get_ip_port(),遍历错误。具体原因为："+str(e)
                pass
            except BaseException, e:
                print "出错于get_ip_port(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_port.append(port)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip端口"


    def get_ip_address(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                address = str(soup.find("tr").findAll("td")[3].get_text()).replace("\n","")
            except IndexError, e:
               # print "出错于get_ip_address(),遍历错误。具体原因为："+str(e)
                pass
            except BaseException, e:
                print "出错于get_ip_address(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_address.append(address)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip地址"

    def get_ip_type(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                type = soup.find("tr").findAll("td")[5].get_text()
            except IndexError, e:
                #print "出错于get_ip_type(),遍历错误。具体原因为："+str(e)
                pass
            except BaseException, e:
                print "出错于get_ip_type(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_type.append(type)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip类型"

    def get_ip_internet_speed(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                internet_speed = soup.find("tr").findAll("td")[6].find({"div", "title"}).attrs["title"]
            except IndexError, e:
                #print "出错于get_ip_internet_speed(),遍历错误。具体原因为："+str(e)
                pass
            except BaseException, e:
                print "出错于get_ip_internet_speed(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_internet_speed.append(internet_speed)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip速度"

    def get_ip_connect_time(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                connect_time = soup.find("tr").findAll("td")[7].find({"div", "title"}).attrs["title"]
            except IndexError, e:
                #print "出错于get_ip_connect_time(),遍历错误。具体原因为："+str(e)
                pass
            except BaseException, e:
                print "出错于get_ip_connect_time(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_connect_time.append(connect_time)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip连接时间"

    def get_ip_survival_time(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                survival_time = soup.find("tr").findAll("td")[8].get_text()
            except IndexError, e:
                #print "出错于get_ip_survival_time(),遍历错误。具体原因为："+str(e)
                pass
            except BaseException, e:
                print "出错于get_ip_survival_time(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_survival_time.append(survival_time)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip存活时间"

    def get_ip_check_date(self):
        time1 = time.time()
        for html in self.htmls:
            try:
                soup = BeautifulSoup(str(html), 'lxml')
                check_date = soup.find("tr").findAll("td")[9].get_text()
            except IndexError, e:
                #print "出错于get_ip_check_date(),遍历错误。具体原因为："+str(e)
                pass
            except BaseException, e:
                print "出错于get_ip_check_date(),程序父错误。具体原因为:"+str(e)
            else:
                self.list_check_date.append(check_date)
        time2 = time.time()
        #print "用时"+str(time2-time1)+'秒'
        #print "已获取所有ip验证时间"

    def get_all_data(self):
        self.get_url()
        #print "已获取所有链接"
        self.get_Html()
        #print "已获取所有页面源码"

        # time1 = time.time()
        # threads = []
        # t1 = threading.Thread(target=self.get_ip)
        # threads.append(t1)
        # t2 = threading.Thread(target=self.get_ip_port)
        # threads.append(t2)
        # t3 = threading.Thread(target=self.get_ip_address)
        # threads.append(t3)
        # t4 = threading.Thread(target=self.get_ip_type)
        # threads.append(t4)
        # t5 = threading.Thread(target=self.get_ip_internet_speed)
        # threads.append(t5)
        # t6 = threading.Thread(target=self.get_ip_connect_time)
        # threads.append(t6)
        # t7 = threading.Thread(target=self.get_ip_survival_time)
        # threads.append(t7)
        # t8 = threading.Thread(target=self.get_ip_check_date)
        # threads.append(t8)
        # for t in threads:
        #     t.setDaemon(True)
        #     t.start()
        # t.join()
        # time2 = time.time()
        # print "用时"+str(time2-time1)+'秒'

        time1 = time.time()
        self.get_ip()
        self.get_ip_port()
        self.get_ip_address()
        self.get_ip_type()
        self.get_ip_internet_speed()
        self.get_ip_connect_time()
        self.get_ip_survival_time()
        self.get_ip_check_date()
        time2 = time.time()
        print "获取ip数据用时:"+str(time2-time1)+'秒'

        # for i in range(0, len(self.htmls)):
        #     try:
        #         print self.list_ip[i]
        #         print self.list_port[i]
        #         print self.list_address[i]
        #         print self.list_type[i]
        #         print self.list_internet_speed[i]
        #         print self.list_connect_time[i]
        #         print self.list_survival_time[i]
        #         print self.list_check_date[i]
        #     except BaseException,e:
        #         print e

    def get_ip_and_port(self):
        self.get_all_data()
        session = requests.session()
        session.headers = headers
        for num in range(0, len(self.htmls)):
            try:
                proxies = {
                    'http':'{0}:{1}'.format(self.list_ip[num], self.list_port[num]),
                    'https':'{0}:{1}'.format(self.list_ip[num], self.list_port[num]),
                }
                session.proxies=proxies
                session.get("http://1212.ip138.com/ic.asp", timeout=1)
                #soup = BeautifulSoup(request.content, 'lxml')
                # print soup.get_text()
                self.ip_and_port = self.list_ip[num]+":"+self.list_port[num]
                print "ip和端口地址为：" + self.ip_and_port
                print "服务器地址为:{0},安全类型为:{1},连接速度为:{2},连接时间为:{3},存活时间为:{4},验证时间为:{5},".format(
                        self.list_address[num], self.list_type[num], self.list_internet_speed[num],
                        self.list_connect_time[num], self.list_survival_time[num], self.list_check_date[num])
                break
            except requests.ConnectionError, e:
                #print "出错于ip_test()，网络问题异常，如DNS查询失败、拒绝连接等。具体原因为:"+str(e)
                print "此ip质量太差，正在获取下一个ip"
            except BaseException,e:
                #print e
                print "此ip质量太差，正在获取下一个ip"

    def getIpAndPortInList(self, url_lsit = []):

        bad_proxies = []
        session = requests.session()
        session.headers = headers
        for num in range(0, len(self.htmls)):
            try:
                proxies = {
                    'http':'{0}:{1}'.format(self.list_ip[num], self.list_port[num]),
                    'https':'{0}:{1}'.format(self.list_ip[num], self.list_port[num]),
                }
                if proxies in bad_proxies:
                    error = 1/0
                else:
                    session.proxies=proxies
                if url_lsit:
                    print "使用已获取的链接列表验证并获取新ip中..."
                    for url in url_lsit:
                        session.get(url, timeout=3)
                        break
                else:
                    while True:
                        session.get("http://1212.ip138.com/ic.asp", timeout=1)
                        break
                #soup = BeautifulSoup(request.content, 'lxml')
                # print soup.get_text()
                self.ip_and_port = self.list_ip[num]+":"+self.list_port[num]
                print "ip和端口地址为：" + self.ip_and_port
                print "服务器地址为:{0},安全类型为:{1},连接速度为:{2},连接时间为:{3},存活时间为:{4},验证时间为:{5},".format(
                        self.list_address[num], self.list_type[num], self.list_internet_speed[num],
                        self.list_connect_time[num], self.list_survival_time[num], self.list_check_date[num])
                break
            except ZeroDivisionError:
                print "去除重复的低质量ip"
            except requests.ConnectionError, e:
                #print "出错于ip_test()，网络问题异常，如DNS查询失败、拒绝连接等。具体原因为:"+str(e)
                bad_proxies.append(proxies)
                print "此ip经验证后质量太差，正在获取下一个ip"
            except BaseException,e:
                bad_proxies.append(proxies)
                print "获取此ip报错，正在获取下一个ip，报错原因为:"+str(e)

# if __name__ == '__main__':
#     p = Proxies()
#     p.ip_test()





